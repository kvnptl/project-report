%!TEX root = ../report.tex
\documentclass[report.tex]{subfiles}
\begin{document}
    \begin{abstract}
        % Background or Introduction: Briefly introduce the problem your research addresses. This might include the challenges in object detection under adverse weather conditions and the significance of multimodal sensor fusion in overcoming these challenges.

% - Object detection is a crucial component in the field of perceiving the environment. Recently there has an increase of deep learning based approach to solve the object detection task. But majority of the research is focused on detection with camera data. 
% - For an autonomous vehicle, outdoor perception is a critical component. But the most of the datasets are available in clear weather conditions. When it comes to adverse weather conditions, where significant ambient light is present, camera only methods fail to work well. Hence, there is a need for multimodal sensor fusion to overcome these limitations in adverse weather conditions. Sensors like LiDAR can sense in dark and low light conditions, and Radar can sense through fog, snow, rain, etc. 
% - Having multisensor suit for a vehicle can enhance the robustness of a perception system in all weather conditions.  

% Objectives or Aims: Clearly state the primary goal of your research. This could involve enhancing object detection accuracy in challenging weather conditions through the integration of multiple sensor data.

% - Hence, the primary goal of the research is to explore the fusion of multimodal sensor data using advance deep learning techniques in adverse weather conditions. The aim is to improve the perception system in inclement weather conditions.

% Methods: Describe the approach or methodology you used in your study. This might involve the types of sensors used, the data fusion technique, and any unique algorithms or models developed.

% - For this reason, this study conducts experiments with three main sensors camera, LiDAR, and Radar. In order to fuse the multimodal data into the same network architecture, we have explored the early fusion vs. middle fusion vs. tightly-coupled fusion approaches. This project mainly focuses on the tightly-coupled fusion approach compared to the existing best performing fusion strategy which is middle fusion approach. We have conducted experiments on two publicly available datasets namely nuScenes and DENSE. The DENSE dataset includes extreme weather conditions to test the robustness of the sensor fusion technique.   

% Results: Summarize the key findings of your research. This could include improvements in detection accuracy, the efficiency of the sensor fusion technique, or any comparative analysis with traditional methods.

% - From our extensive study with multisensor and with adverse weather conditions data, we conclude two key findings. First, having fusion of complementary sensors including camera, LiDAR, and Radar significantly improves the detection accuracy in all weather conditions. Second, the use of tightly-coupled fusion approach outperforms the existing middle fusion or feature fusion approach. We have done both qualitative and quantitative analysis to prove our findings. 

% Conclusion: Conclude with the implications of your findings and their significance in the field of computer vision and object detection.
% With this...... 

% Future Work: Optionally, you can mention potential areas for future research or further applications of your work.


In the field of autonomous vehicles, object detection is a critical component, especially in perceiving the environment under adverse weather conditions. Traditional methods, primarily focused on camera data, face significant limitations in such scenarios. This research aims to address these challenges through the exploration of multimodal sensor fusion, incorporating Cameras, LiDAR, and Radar, to improve detection accuracy in inclement weather. The study primarily focuses on a tightly-coupled fusion approach, contrasted against the existing middle fusion strategy, with experiments conducted using the nuScenes and DENSE datasets, the latter featuring extreme weather conditions. The findings indicate that the integration of complementary sensors substantially enhances detection accuracy across various weather conditions, and that the tightly-coupled fusion approach outperforms the middle fusion method. Both qualitative and quantitative analyses support these conclusions, highlighting the effectiveness of this approach in the advancement of object detection technologies in autonomous vehicles. This research provides significant insights into the robustness of sensor fusion techniques, offering substantial contributions to the fields of computer vision and autonomous vehicle technology.


% In the evolving landscape of autonomous vehicles, object detection plays a pivotal role in environmental perception. Traditional approaches predominantly utilize camera data, demonstrating limitations under adverse weather conditions due to compromised ambient light. This research addresses this gap by investigating multimodal sensor fusion, leveraging the complementary strengths of cameras, LiDAR, and Radar to enhance object detection in challenging weather scenarios.

% The primary objective of this study is to augment object detection accuracy in inclement weather through advanced deep learning techniques that integrate multimodal sensor data. Focusing on tightly-coupled fusion, the research contrasts this approach against the prevalent middle fusion method. Experimental analysis utilizes the nuScenes and DENSE datasets, with the latter specifically designed to challenge sensor fusion techniques under extreme weather conditions.

% Our findings reveal two critical insights: First, the integration of complementary sensors markedly improves detection accuracy across diverse weather conditions. Second, the tightly-coupled fusion approach notably surpasses the middle fusion method in performance. These conclusions are supported through both qualitative and quantitative evaluations.

% In summary, this research substantiates the efficacy of tightly-coupled, data-driven multimodal sensor fusion in enhancing object detection in adverse weather conditions. The implications of these findings are profound, offering a substantial advancement in the field of computer vision and autonomous vehicle technology.

    \end{abstract}
\end{document}
